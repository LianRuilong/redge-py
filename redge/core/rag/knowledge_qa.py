import ollama
import time
from redge.core.rag.knowledge_query import KnowledgeQuery

class KnowledgeQA:
    """
    çŸ¥è¯†é—®ç­”æ¨¡å—ï¼š
    1. æ¥æ”¶ç”¨æˆ·é—®é¢˜
    2. é€šè¿‡ KnowledgeQuery è·å–æ–‡æ¡£ç‰‡æ®µ
    3. ç»„ç»‡ promptï¼Œè°ƒç”¨ Ollama æœ¬åœ°å¤§æ¨¡å‹
    4. è¿”å›ç­”æ¡ˆå’Œå»é‡åçš„å‚è€ƒæ–‡æ¡£åˆ—è¡¨
    """

    def __init__(self, model="mistral"):
        """
        åˆå§‹åŒ– Ollama æ¨¡å‹
        :param model: Ollama æœ¬åœ°éƒ¨ç½²çš„ LLM åç§°
        """
        self.model = model
        self.knowledge_query = KnowledgeQuery()

    def generate_prompt(self, question, documents):
        """
        ç”Ÿæˆ LLM è¾“å…¥çš„ Prompt
        :param question: ç”¨æˆ·é—®é¢˜
        :param documents: ç›¸å…³æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        :return: ç”Ÿæˆçš„ prompt
        """
        doc_texts = "\n\n".join([f"ç‰‡æ®µ {i+1}: {doc['content']}" for i, doc in enumerate(documents)])
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

### ç”¨æˆ·é—®é¢˜ï¼š
{question}

### ç›¸å…³æ–‡æ¡£ï¼š
{doc_texts}

è¯·ç»¼åˆè¿™äº›ä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚
"""
        return prompt

    def ask(self, question):
        """
        æ‰§è¡Œé—®ç­”æµç¨‹ï¼š
        1. æŸ¥è¯¢çŸ¥è¯†åº“
        2. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
        3. è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ŒåŒ…æ‹¬å›ç­”å†…å®¹å’Œå»é‡åçš„å¼•ç”¨æ–‡æ¡£
        """
        # 1ï¸âƒ£ **è°ƒç”¨ knowledge_query è·å–ç›¸å…³æ–‡æ¡£**
        documents = self.knowledge_query.query(question, top_k=3)
        if not documents:
            return {
                "answer": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
                "references": []
            }

        # 2ï¸âƒ£ **ç”Ÿæˆ Prompt**
        prompt = self.generate_prompt(question, documents)

        # 3ï¸âƒ£ **è°ƒç”¨ Ollama LLM**
        response = ollama.chat(model=self.model, messages=[{"role": "user", "content": prompt}])
        answer = response['message']['content']

        # 4ï¸âƒ£ **å»é‡å¹¶æ•´ç†å¼•ç”¨æ–‡æ¡£**
        seen_files = set()  # å­˜å‚¨å·²è¾“å‡ºçš„ `file_path`
        references = []
        for doc in documents:
            if doc['file_path'] not in seen_files:
                references.append({"file_name": doc["file_name"], "file_path": doc["file_path"]})
                seen_files.add(doc['file_path'])  # æ ‡è®°ä¸ºå·²è¾“å‡º

        # 5ï¸âƒ£ **è¿”å› JSON ç»“æ„åŒ–æ•°æ®**
        return {
            "answer": answer,
            "references": references
        }

    def ask_stream(self, question):
        """
        **æµå¼å›ç­”**
        1. é€æ­¥è·å– LLM ç”Ÿæˆçš„ token
        2. ä½¿ç”¨ yield é€æ­¥è¿”å›æ•°æ®ï¼ˆé€‚ç”¨äº FastAPI Streamingï¼‰
        """
        # 1ï¸âƒ£ **è°ƒç”¨ knowledge_query è·å–ç›¸å…³æ–‡æ¡£**
        documents = self.knowledge_query.query(question, top_k=3)
        if not documents:
            yield "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹\n"
            return

        # 2ï¸âƒ£ **ç”Ÿæˆ Prompt**
        prompt = self.generate_prompt(question, documents)

        # 3ï¸âƒ£ **æµå¼è°ƒç”¨ Ollama LLM**
        response = ollama.chat(model=self.model, messages=[{"role": "user", "content": prompt}], stream=True)
        
        for chunk in response:
            yield chunk["message"]["content"]

        # 4ï¸âƒ£ **è¾“å‡ºå‚è€ƒæ–‡æ¡£**
        yield "\n\nğŸ“‚ **å‚è€ƒæ–‡æ¡£:**\n"
        seen_files = set()
        for doc in documents:
            if doc["file_path"] not in seen_files:
                yield f"ğŸ“„ {doc['file_name']} | ğŸ“ {doc['file_path']}\n"
                seen_files.add(doc["file_path"])

if __name__ == "__main__":
    qa = KnowledgeQA(model="deepseek-r1:1.5b")  # é€‰æ‹©æœ¬åœ° LLM
    query_text = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
    response = qa.ask(query_text)

    # æ‰“å°ç»“æ„åŒ–è¿”å›æ•°æ®
    print("\nğŸ¤– **å›ç­”:** \n")
    print(response["answer"])
    print("\nğŸ“‚ **å‚è€ƒæ–‡æ¡£:**")
    for ref in response["references"]:
        print(f"ğŸ“„ {ref['file_name']} | ğŸ“ {ref['file_path']}")